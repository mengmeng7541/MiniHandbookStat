[
["index.html", "A Mini Handbook of Statistical Analyses Prepare", " A Mini Handbook of Statistical Analyses Charlotte 2019-04-04 Prepare Definitions Population: the collection of all individuals or items under consideration in a statistical study. (Weiss, 1999) Sample: the part of the population from which information is collected. (Weiss, 1999) Parameter: A descriptive measure for a population, ex. \\(\\mu, \\sigma\\) Statistic: A descriptive measure for a sample, ex. \\(\\bar{x}, s\\) "],
["basic-intro.html", "§ Chapter 1 Basic Intro 1.1 Descriptive statistics 1.2 Normality Test", " § Chapter 1 Basic Intro 1.1 Descriptive statistics This famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. require(datasets) str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## library(&quot;ggplot2&quot;) bp &lt;- ggplot(iris, aes(Species, Sepal.Length, color=Species) )+ geom_boxplot(outlier.shape = 18, outlier.size = 4) library(&quot;wesanderson&quot;) bp + scale_color_manual(values=wes_palette(n=3,name = &quot;Moonrise3&quot;)) Figure 1.1: Iris boxplot hp &lt;- ggplot(iris, aes(x=Sepal.Length, stat(density), col=Species, fill=Species))+ geom_histogram(alpha=0.5)+ geom_density(alpha=0.6) hp + scale_color_manual(values=wes_palette(n=3,name = &quot;Moonrise3&quot;))+ scale_fill_manual(values=wes_palette(n=3,name = &quot;Moonrise3&quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.2: Iris histogram plot p &lt;- ggplot(iris, aes(Sepal.Length, Sepal.Width))+ geom_point(size=3, aes(col=Species, shape=Species)) p + scale_color_manual(values=wes_palette(n=3,name = &quot;Moonrise3&quot;)) Figure 1.3: Iris ggplot Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure1.2 &amp; Figure 1.3. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 1.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 1.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa 1.2 Normality Test The Shapiro-Wilk Test For Normality shapiro.test(iris$Sepal.Length) ## ## Shapiro-Wilk normality test ## ## data: iris$Sepal.Length ## W = 0.97609, p-value = 0.01018 Shapiro-Wilk, p-value is 0.01018 &lt;0.05. So we reject the null hypothesis. The data are not from a normal population. shapiro.test(iris$Sepal.Width) ## ## Shapiro-Wilk normality test ## ## data: iris$Sepal.Width ## W = 0.98492, p-value = 0.1012 Shapiro-Wilk, p-value is 0.1012 &gt;0.05. So we accept the null hypothesis. The data come from a normal population. "],
["students-t-test.html", "§ Chapter 2 Student’s t-test 2.1 One sample t-test 2.2 Paired t-test 2.3 Two samples t-test", " § Chapter 2 Student’s t-test Hypothesis testing is a statistical method that is used in making statistical decisions using experimental data. Hypothesis Testing is basically an assumption that we make about the population parameter. 2.1 One sample t-test Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p308; 12.6】 The yield of alfalfa from a random sample of six test plots in 1.4, 1.6, 0.9, 1.9, 2.2, and 1.2 tons per acre. (a). Check whether these data can be looked upon as a sample from a normal population (b). If so, test at the 0.05 level of significance whether this supports the contention that the average yield for this kind of alfalfa is 1.5 tons per acre. Assumption: the population we are sampling has roughly the shape of a normal distribution. asfalfa &lt;- c(1.4, 1.6, 0.9, 1.9, 2.2, 1.2 ) t.test(asfalfa, mu=1.5) ## ## One Sample t-test ## ## data: asfalfa ## t = 0.17303, df = 5, p-value = 0.8694 ## alternative hypothesis: true mean is not equal to 1.5 ## 95 percent confidence interval: ## 1.038130 2.028536 ## sample estimates: ## mean of x ## 1.533333 \\(H_0: \\mu =1.5\\) v.s. \\(H_a: \\mu \\not= 1.5\\) \\(\\alpha=0.05\\) Reject the null hypothesis if \\(t\\le-2.571\\) or \\(t\\ge 2.571\\), where \\(t = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}}\\), and \\(2.571\\) is the value of \\(t_{0.025}\\) for \\(6-1=5\\) degrees of freedom. Calculating the formula for \\(t\\), \\(t = \\frac{1.5333-1.5}{0.47188/\\sqrt{6}}\\approx 0.173\\) Since \\(t=0.173\\) falls between \\(-2.571\\) and \\(2.571\\), the \\(H_0: \\mu =1.5\\) can not be rejected; in other word, the data tend to support the contention that the average yield of the given kind of alfalfa is 1.5 tons per acre. 2.2 Paired t-test Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p317; 12.9】 Following are the average weekly losses of worker hours due to accidents in ten industrial plants before and after the installation of an elaborate safety program: 45 and 36 、 73 and 60 、 46 and 44 、 124 and 119 、 33 and 35 、 57 and 51 、 83 and 77 、 34 and 29 、 26 and 24 、17 and 11 Use the 0.05 level of significance to test whether the safety program is effective. Assumption: the population we are sampling has roughly the shape of a normal distribution. x &lt;- c(45, 73, 46, 124, 33, 57, 83, 34, 26, 17) y &lt;- c(36, 60, 44, 119, 35, 51, 77, 29, 24, 11) t.test(x, y, paired=TRUE, alternative=&#39;greater&#39; ) ## ## Paired t-test ## ## data: x and y ## t = 4.0333, df = 9, p-value = 0.001479 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 2.836619 Inf ## sample estimates: ## mean of the differences ## 5.2 The differences between the respective pairs are 9, 13, 2, 5, -2, 6, 6, 5, 2, and 6. \\(H_0: \\mu =0\\) v.s. \\(H_a: \\mu \\ge 0\\) \\(\\alpha=0.05\\) Reject the null hypothesis if \\(t\\ge 1.833\\), where \\(t = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}}\\), and \\(1.8333\\) is the value of \\(t_{0.05}\\) for \\(10-1=9\\) degrees of freedom. Calculating the formula for \\(t\\), \\(t = \\frac{5.2-0}{4.077/\\sqrt{10}}\\approx 4.033\\) Since \\(t=4.033\\) falls exceeds \\(1.833\\), the \\(H_0: \\mu =0\\) must be rejected; in other word, we have shown that the industrial safety program is effective. 2.3 Two samples t-test Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p315; 12.8】 The following random samples are measurements of the heat-producing capacity ( in millions of calories pet ton ) of coal from two mines : Mine1: 8380 8180 8500 7840 7990 Mine2: 7660 7510 7910 8070 7790 Use the 0.05 level of significance to test whether the difference between the means of these two samples is significant. Assumption: 1.The population we are sampling has roughly the shape of a normal distribution. 2.The two samples have equal standard deviations. 3.The two samples are independent M1 &lt;- c(8380, 8180, 8500, 7840, 7990) M2 &lt;- c(7660, 7510, 7910, 8070, 7790) t.test(M1, M2, var.equal=TRUE) ## ## Two Sample t-test ## ## data: M1 and M2 ## t = 2.5118, df = 8, p-value = 0.03627 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 31.95248 748.04752 ## sample estimates: ## mean of x mean of y ## 8178 7788 We suspect the assumption of \\(\\sigma_1^2=\\sigma_2^2\\) \\(H_0: \\mu_1 =\\mu_2\\) v.s. \\(H_a: \\mu_1 \\neq \\mu_2\\) \\(\\alpha=0.05\\) Reject the null hypothesis if \\(t\\le-2.306\\) or \\(t\\ge2.306\\) , where \\(t\\) is given by the formula above, and \\(2.306\\) is the critical value . Calculating the formula for \\(t\\), \\(t=\\frac{(8178-7788)}{245.5\\sqrt{\\frac{1}{5}+\\frac{1}{5}}}\\approx 2.51\\) Since \\(t = 2.51\\) falls exceeds \\(2.306\\), the \\(H_0: \\mu_1 =\\mu_2\\) must be rejected; in other words, we conclude that the difference between the two sample means is significant. "],
["one-way-anova.html", "§ Chapter 3 One-Way ANOVA", " § Chapter 3 One-Way ANOVA Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p375; 15.3】 A laboratory technician wants to compare the breaking strength of three kinds of thread and originally he had planned to repeat each determination six times. Not having enough time, however, he has to base his analysis on the following results (in ounces): Thread1 18.0 16.4 15.7 19.6 16.5 18.2 Thread2 21.1 17.8 18.6 20.8 17.9 19.0 Thread3 16.5 17.8 16.1 Assuming that these data constitute random samples from three normal populations with the same standard deviation, perform an analysis of variance to test at the 0.05 level of significance whether the differences among the sample means are significant. Assumption: The data constitute random samples from normal populations. These normal populations all have the same variance. Samples are independent. lab &lt;- read.table(&quot;C://data/aov1.txt&quot;, header=TRUE) aov.lab = aov(strength ~ group, data=lab) summary(aov.lab) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 2 15.12 7.560 4.061 0.045 * ## Residuals 12 22.34 1.862 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TukeyHSD(aov.lab) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = strength ~ group, data = lab) ## ## $group ## diff lwr upr p adj ## b-a 1.8 -0.3016189 3.901619 0.0967824 ## c-a -0.6 -3.1739470 1.973947 0.8111220 ## c-b -2.4 -4.9739470 0.173947 0.0684768 \\(H_0: \\mu_1 =\\mu_2=\\mu_3\\) v.s. \\(H_a\\):The \\(\\mu&#39;s\\) are not all equal. \\(\\alpha=0.05\\) Reject the null hypothesis if \\(F&gt;F_{0.05}(2,12)=3.89\\) , where\\(K-1=2\\) , and \\(N-K=15-3=12\\). Since \\(F=4.06\\) exceeds \\(3.89\\), the null hypothesis must be rejected; in other words, we conclude that there is a difference in the strength of the three kinds of thread. Now we want to know where inequalities exists among the different 3 means. Tukey’s Method is to test all possible pairwise differences of means to determine if at least one difference is significantly different from 0. Assume the 90% confidence coefficient, The simultaneous pairwise comparisons indicate that the differences of \\(\\mu_3-\\mu_1\\) are not significantly different from 0 (their confidence intervals include 0). "],
["chi-square-test.html", "§ Chapter 4 Chi-Square Test 4.1 Cross Table 4.2 Test of Goodness of Fit 4.3 Test of Homogeneity 4.4 Test of Independence", " § Chapter 4 Chi-Square Test Some significant applications are demonstrated in this chapter. 4.1 Cross Table Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p340】 Suppose we want to investigate whether there is a relationship between the test scores of persons who have gone through a certain job-training and their subsequent performance on the job. Performance Poor Fair Good Below average 67 64 25 Average 42 76 56 Above average 10 23 37 chisq.test(PT, correct=FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: PT ## X-squared = 41.014, df = 4, p-value = 2.67e-08 \\(H_0\\):Test scores and on-the-job performance are independent. \\(H_A\\):Test scores and on-the-job performance are not independent. \\(\\alpha=0.01\\) Reject the null hypothesis if \\(\\chi^2 &gt; \\chi_{0.01}^2(4)=13.277\\), where degrees of freedom is \\((3-1)(3-1)=4\\) Since \\(\\chi^2\\approx41.014\\) exceeds \\(13.277\\), the null hypothesis must be rejected; that is, we conclude that there is a relationship between the test scores and on-the-job performance. 4.2 Test of Goodness of Fit Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p351】 The management of an airport wants to check an air-traffic controller’s claim that the number of ratio messages received per minute is a random variable having the Poisson distribution with the mean \\(\\lambda=1.5\\). Number of radio messages Observed frequency 0 70 1 57 2 46 3 20 4 7 x &lt;- 0:4 y&lt;- c(70, 57, 46, 20, 7) q &lt;- ppois(x, lambda=1.5) n &lt;- length(y) p&lt;-numeric(length=5) p[1] &lt;- q[1] p[n] &lt;- 1-q[n-1] for(i in 2:(n-1)) p[i] &lt;- q[i]-q[i-1] chisq.test(y, p=p) ## ## Chi-squared test for given probabilities ## ## data: y ## X-squared = 20.153, df = 4, p-value = 0.0004658 \\(H_0\\):The population smpled has the Poisson distribution with \\(\\lambda=1.5\\). \\(H_A\\):The population smpled has the Poisson distribution with \\(\\lambda \\neq 1.5\\) \\(\\alpha=0.01\\) Reject the null hypothesis if \\(\\chi^2 &gt; \\chi_{0.01}^2(4)=13.277\\), degree of freedom is \\(k-m-1=5-0-1=4\\). Where \\(k=5, m=0\\)(the number of parameters of the probability distribution). Since \\(\\chi^2 \\approx 20.148\\) exceeds \\(13.277\\), the null hypothesis must be rejected; we conclude that either the population does not have a Poisson distribution or it has a Poisson distribution with \\(\\lambda\\) different from 1.5. 4.3 Test of Homogeneity Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p342】 With reference to the problem dealing with the factors contribution most to one’s well-being, test at the 0.01 level of significance whether for each of the three alternatives the probabilities are the same for persons who are single, married, or widowed or divorced. Marry Single Married Widowed or divorced Health1 41 27 12 Health2 49 50 21 Health3 42 33 25 marriage &lt;- c(41, 49, 42, 27, 50, 33, 12, 21, 25) dim(marriage) &lt;- c(3, 3) chisq.test(marriage, correct=FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: marriage ## X-squared = 5.3371, df = 4, p-value = 0.2544 \\(H_0\\):\\(p_{i1}=p_{i2}=p_{i3}\\) for \\(i=1,2,3\\). \\(H_A\\):\\(p_{i1}, p_{i2}, p_{i3}\\) are not all equal for at least one value of \\(i\\). \\(\\alpha=0.01\\) Reject the null hypothesis if \\(\\chi^2 &gt; \\chi_{0.01}^2(4)=13.277\\), where degrees of freedom is \\((3-1)(3-1)=4\\). Since \\(\\chi^2\\approx5.337\\) is less than 13.277, the null hypothesis must be accepted; that is ,we conclude that for each of the three alternatives the probabilities are the same for persons who are single, married, or widowed or divorced. 4.4 Test of Independence Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p348; 14.23】 A sample survey conducted at a large state university, whether there is a relationship between students’ interest and ability in studying a foreign language. Use the 0.01 level of significance. Ability1 Ability2 Ability3 Interest1 28 17 15 Interest2 20 40 20 Interest3 12 28 40 university &lt;- c(28, 20, 12, 17, 40, 28, 15, 20, 40) dim(university) &lt;- c(3, 3) chisq.test(university, correct=FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: university ## X-squared = 26.774, df = 4, p-value = 2.209e-05 \\(H_0\\):Student’s interest and ability are independent. \\(H_A\\):Student’s interest and ability are not independent. \\(\\alpha=0.01\\) Reject the null hypothesis if \\(\\chi^2 &gt; \\chi_{0.01}^2(4)=13.277\\), where degrees of freedom is \\((3-1)(3-1)=4\\). Since \\(\\chi^2\\approx 26.774\\) is greater than 13.277, the null hypothesis must be rejected; that is , we conclude that there is a relationship between student’s interest and ability in studying a foreign language. "],
["regression-analysis.html", "§ Chapter 5 Regression Analysis 5.1 Linear Regression 5.2 Multiple Regression Analysis 5.3 Logistic Regression Anaylsis", " § Chapter 5 Regression Analysis 5.1 Linear Regression Data :【Modern Elementary Statistics (11th Edition): John E. Freund; p418; 16.7】 The following data show the average number of hours that six students spent on homework per week and their grade-point indexes for the courses they took in that semester: x &lt;- c(15, 28, 13, 20, 4, 10) y &lt;- c(2, 2.7, 1.3, 1.9, 0.9, 1.7) summary(lm(y~1+x)) ## ## Call: ## lm(formula = y ~ 1 + x) ## ## Residuals: ## 1 2 3 4 5 6 ## 0.25000 0.05814 -0.31279 -0.19302 -0.09535 0.29302 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.72093 0.24641 2.926 0.04300 * ## x 0.06860 0.01467 4.678 0.00946 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.272 on 4 degrees of freedom ## Multiple R-squared: 0.8455, Adjusted R-squared: 0.8068 ## F-statistic: 21.88 on 1 and 4 DF, p-value: 0.009461 plot(x, y) abline( lm(y~1+x), col=3) The regression equation is \\(Y=0.721+0.069x\\) \\(H_0\\): \\(\\beta_1=0\\) \\(H_A\\): \\(\\beta_1\\neq0\\) \\(\\alpha=0.01\\) The value of the test statistic, \\(t=4.698, p-value=0.009 &lt; 0.01\\). We conclude that \\(\\beta_1\\neq0\\) or there is a linear association between six students spent on homework per week and their grade-point indexes. By ANOVA table, since \\(F=21.884, p-value=0.09 &lt; 0.01\\).We conclude that \\(\\beta_1\\neq0\\). This is the same result as when the t test. In simple linear regression. \\(F=t^2 ; F(1-\\alpha; 1, n-2)=[t(1-\\alpha/2; n-2)]^2\\) 5.2 Multiple Regression Analysis Data :【Modern Elementary Statistics (11th Edition): John E. Freund; p424】 The following data show the number of bedrooms, the number of baths, and the prices at which eight one-family houses sold recently in a certain community: mltireg &lt;- data.frame( x1 &lt;- c(3, 2, 4, 2, 3, 2, 5, 4), x2 &lt;- c(2, 1, 3, 1, 2, 2, 3, 2), y &lt;- c(143800, 109300, 158800, 109200, 154700, 114900, 188400, 142900) ) lm.m &lt;- lm (y ~x1+x2, data=mltireg) summary(lm.m) ## ## Call: ## lm(formula = y ~ x1 + x2, data = mltireg) ## ## Residuals: ## 1 2 3 4 5 6 7 8 ## 5644 -869 -7343 -969 16544 -6504 5505 -12008 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65430 12134 5.392 0.00296 ** ## x1 16752 6636 2.524 0.05288 . ## x2 11234 9885 1.137 0.30724 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10750 on 5 degrees of freedom ## Multiple R-squared: 0.8941, Adjusted R-squared: 0.8517 ## F-statistic: 21.1 on 2 and 5 DF, p-value: 0.003653 This tells us that (in the given community at the time the study was being made) each extra bedroom added on the average 16752, and each bath 11234,to the sales price of a house. The regression equation is \\(\\hat{y}= 65430+16752x_1+11234x_2\\) 5.3 Logistic Regression Anaylsis Data: https://stats.idre.ucla.edu/r/dae/logit-regression/ Example 1: Suppose that we are interested in the factors that influence whether or not a political candidate wins an election. The outcome (response) variable is binary (0/1); win or lose. The predictor variables of interest are: the amount of money spent on the campaign, the amount of time spent campaigning negatively and whether or not the candidate is an incumbent. Because the response variable is binary we need to use a model that handles 0/1 variables correctly. logit &lt;- read.table(&quot;c://data/logit.txt&quot;, header=TRUE) glm.logit &lt;- glm(admit ~ gre+ topnotch+ gpa, family=binomial, data=logit) summary(glm.logit) ## ## Call: ## glm(formula = admit ~ gre + topnotch + gpa, family = binomial, ## data = logit) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3905 -0.8836 -0.7137 1.2745 1.9572 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.600813 1.096379 -4.196 2.71e-05 *** ## gre 0.002477 0.001070 2.314 0.0207 * ## topnotch 0.437224 0.291853 1.498 0.1341 ## gpa 0.667556 0.325259 2.052 0.0401 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 478.13 on 396 degrees of freedom ## AIC: 486.13 ## ## Number of Fisher Scoring iterations: 4 Both gre and gpa are statistically significant while topnotch is not. The interpretation of the coefficients can be awkward. For example, for a one unit increase in gpa, the log odds of being admitted to graduate school (versus not being admitted) increases by 0.668. For this reason, many researchers prefer to exponentiate the coefficients and interpret them as odds-ratios. For example, we can say that for a one unit increase in gpa, the odds of being admitted to graduate school (versus not being admitted) increased by a factor of 1.949. Since GRE scores do not increase by a single unit (they increase only in units of 10), a one unit increase is meaningless. We can take the odds ratio and raise it to the 10th power, e.g.\\(1.002^{10}=1.020181\\) , and say for a 10 unit increase in GRE score, the odds of admission to graduate school increased by a factor of 1.02. "],
["categorical-data-analysis.html", "§ Chapter 6 Categorical Data Analysis", " § Chapter 6 Categorical Data Analysis Data: A r x c table 【Agresti (2002, p. 57) Job Satisfaction 】 The variables are income and job satisfaction, measured for the black males in a national (U.S.) sample. Both classifications are ordinal, job satisfaction with the categories very dissatisfied(VD), little dissatisfied(LD), moderately satisfied(MS), and very satisfied(VS). Job Satisfaction Very Dissatisfied Little Dissatisfied Moderately Satisfied Very Satisfied &lt; 15k 1 3 10 6 15k~25k 2 3 10 7 25k~40k 1 6 14 12 &gt; 40k 0 1 9 11 Job &lt;- matrix(c(1,2,1,0, 3,3,6,1, 10,10,14,9, 6,7,12,11), 4, 4, dimnames = list(income=c(&quot;&lt; 15k&quot;, &quot;15k~25k&quot;, &quot;25k~40k&quot;, &quot;&gt; 40k&quot;), satisfaction=c(&quot;Very Dissatisfied&quot;, &quot;Little Dissatisfied&quot;, &quot;Moderately Satisfied&quot;, &quot;Very Satisfied&quot;))) fisher.test(Job, simulate.p.value=TRUE, B=1e5) ## ## Fisher&#39;s Exact Test for Count Data with simulated p-value (based ## on 1e+05 replicates) ## ## data: Job ## p-value = 0.7804 ## alternative hypothesis: two.sided The p-value&gt;0.05, so we can accept Null hypothesis, income and job satisfaction are independent. "],
["nonparametric-statistics.html", "§ Chapter 7 Nonparametric Statistics 7.1 Kolmogorov-Smirnov Test 7.2 Wilcoxon Signed Ranks Test 7.3 Sign Test 7.4 Mann-Whitney Test ( Wilcoxon rank-sum test) 7.5 Two sample Kolmogorov-Smirnov Test 7.6 Kruskal-Wallis Test", " § Chapter 7 Nonparametric Statistics 7.1 Kolmogorov-Smirnov Test Data : 【Practical Nonparametric Statistics (3rd Edition) :Wiley Series in Probability and Statistics; p433】 The random sample of size 10 is obtained: 0.621, 0.503, 0.203, 0.477, 0.710, 0.581, 0.329, 0.480, 0.554, 0.382. The null hypothesis is that the distribution function is uniform distribution. size &lt;- c(0.621, 0.503, 0.203, 0.477, 0.710, 0.581, 0.329, 0.480, 0.554, 0.382) ks.test (size, &quot;punif&quot;, 0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: size ## D = 0.29, p-value = 0.3067 ## alternative hypothesis: two-sided \\(H_0\\): The distribution function is uniform distribution. \\(H_A\\): The distribution function is not uniform distribution. \\(\\alpha=0.01\\) By Kolmogorov-Smirnov test, p-value is 0.3067 &gt;0.01, the null hypothesis cannot be rejected; there is no real evidence to indicate that the distribution function is not uniform distribution. 7.2 Wilcoxon Signed Ranks Test Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p317; 12.9】 (we used in paired T test before) Following are the average weekly losses of worker hours due to accidents in ten industrial plants before and after the installation of an elaborate safety program: 45 and 36 、 73 and 60 、 46 and 44 、 124 and 119 、 33 and 35 、 57 and 51 、 83 and 77 、 34 and 29 、 26 and 24 、17 and 11 Based on the Wilcoxon Signed Ranks Test, use 0.05 level of significance that the safety program is effective. x &lt;- c(45, 73, 46, 124, 33, 57, 83, 34, 26, 17) y &lt;- c(36, 60, 44, 119, 35, 51, 77, 29, 24, 11) wilcox.test(x, y, alternative=&quot;greater&quot;, paired=TRUE) ## Warning in wilcox.test.default(x, y, alternative = &quot;greater&quot;, paired = ## TRUE): cannot compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: x and y ## V = 53, p-value = 0.005185 ## alternative hypothesis: true location shift is greater than 0 \\(H_0:\\tilde{\\mu}_D=0\\) (\\(\\tilde{\\mu}_D\\) is the median of the population of differences) \\(H_A:\\tilde{\\mu}_D&gt;0\\) \\(\\alpha=0.05\\) From Wilcoxon Signed Ranks Test, \\(T^+\\)=53 p-value=0.005185. The null hypothesis is must be rejected . We conclude that the safety program is effective. 7.3 Sign Test Data is the same as Wilcoxon Signed Ranks Test x &lt;- c(45, 73, 46, 124, 33, 57, 83, 34, 26, 17) y &lt;- c(36, 60, 44, 119, 35, 51, 77, 29, 24, 11) binom.test(sum(x&gt;y), n=length(x), alternative=&quot;greater&quot;) ## ## Exact binomial test ## ## data: sum(x &gt; y) and length(x) ## number of successes = 9, number of trials = 10, p-value = 0.01074 ## alternative hypothesis: true probability of success is greater than 0.5 ## 95 percent confidence interval: ## 0.6058367 1.0000000 ## sample estimates: ## probability of success ## 0.9 \\(H_0:\\tilde{\\mu}_D=0\\) (\\(\\tilde{\\mu}_D\\) is the median of the population of differences) \\(H_A:\\tilde{\\mu}_D&gt;0\\) \\(\\alpha=0.05\\) The test statistic is the number of plus signs, namely the number of industrial plants in which the average weekly losses in hours of labor has decreased. If the first value is greater than the second, we use “+”. If the first value is smaller than the second, we use “ ”. So we get “ + + + + + + + +”. Thus the test statistic is 9. By Binomial Probability Table, when n=10, p=0.5, the p-value is 0.01+0.001 = 0.011. (dbinom(10, 10, 0.5)+dbinom(9, 10, 0.5)= 0.01074219) Since 0.011 is less than 0.05, the null hypothesis must be rejected. We conclude that the safety program is effective. (The result is the same as Wilcoxon Signed Ranks Test and Paired t-test). 7.4 Mann-Whitney Test ( Wilcoxon rank-sum test) Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p469】 We want to compare the grain size of sand obtained from two different locations on the moon on the basis of the following diameters(in millimeters): Location1: 0.37 0.7 0.75 0.3 0.45 0.16 0.62 0.73 0.33 Location2: 0.86 0.55 0.8 0.42 0.97 0.84 0.24 0.51 0.92 0.62 Use the Mann-Whitney Test at 0.05 level of significance to test whether or not the two samples come from populations with equal means. x.m &lt;- c(0.37, 0.7, 0.75, 0.3, 0.45, 0.16, 0.62, 0.73, 0.33) y.m &lt;- c(0.86, 0.55, 0.8, 0.42, 0.97, 0.84, 0.24, 0.51, 0.92, 0.62) wilcox.test(x.m, y.m, correct=FALSE) ## Warning in wilcox.test.default(x.m, y.m, correct = FALSE): cannot compute ## exact p-value with ties ## ## Wilcoxon rank sum test ## ## data: x.m and y.m ## W = 24.5, p-value = 0.09402 ## alternative hypothesis: true location shift is not equal to 0 \\(H_0:\\mu_1=\\mu_2\\) \\(H_A:\\mu_1\\neq\\mu_2\\) \\(\\alpha=0.05\\) Check the table of Critical Values of U*, when n1=9, n2=10, reject the null hypothesis if \\(U_{0.05}\\le 20\\) . From Mann-Whitney Test, \\(W_1=69, U_1=24\\). Since \\(U=24\\) is greater than 20, the null hypothesis cannot be rejected; we cannot conclude that there is a difference in the mean grain size of sand from the two locations on the moon. 7.5 Two sample Kolmogorov-Smirnov Test Data is the same as Mann-Whitney Test x.m &lt;- c(0.37, 0.7, 0.75, 0.3, 0.45, 0.16, 0.62, 0.73, 0.33) y.m &lt;- c(0.86, 0.55, 0.8, 0.42, 0.97, 0.84, 0.24, 0.51, 0.92, 0.62) ks.test(x.m, y.m) ## Warning in ks.test(x.m, y.m): cannot compute exact p-value with ties ## ## Two-sample Kolmogorov-Smirnov test ## ## data: x.m and y.m ## D = 0.5, p-value = 0.1871 ## alternative hypothesis: two-sided \\(H_0:\\mu_1=\\mu_2\\) \\(H_A:\\mu_1\\neq\\mu_2\\) \\(\\alpha=0.05\\) By Kolmogorov-Smirnov test, p-value is 0.187 &gt;0.05, the null hypothesis cannot be rejected; there is no real evidence to indicate that the mean grain size of sand from the two locations on the moon is difference. 7.6 Kruskal-Wallis Test Data : 【Modern Elementary Statistics (11th Edition): John E. Freund; p475】 Students are randomly assigned to groups that are taught Spanish by three different methods: (1) classroom instruction and language laboratory. (2) only classroom instruction, and (3) only self-study in language laboratory. Following are the final examination scores of samples of students from the three groups: Method1: 94 88 91 74 86 97 Method2: 85 82 79 84 61 72 80 Method3: 89 67 72 76 69 Use the Kruskal-Wallis Test at the 0.05 level of significance to test the null hypothesis that the populations sampled are identical against the alternative hypothesis that their means are not all equal. M1 &lt;- c(94, 88, 91, 74, 86, 97) # Method1 M2&lt;- c(85, 82, 79, 84, 61, 72, 80) # Method2 M3&lt;- c(89, 67, 72, 76, 69) # Method3 kruskal.test(list(M1, M2, M3)) ## ## Kruskal-Wallis rank sum test ## ## data: list(M1, M2, M3) ## Kruskal-Wallis chi-squared = 6.6731, df = 2, p-value = 0.03556 ## Equivalently, M &lt;- c(M1, M2, M3) g &lt;- factor(rep(1:3, c(6, 7, 5)), labels = c(&quot;Method1&quot;, &quot;Method2&quot;, &quot;Method3&quot;)) kruskal.test(M, g) ## ## Kruskal-Wallis rank sum test ## ## data: M and g ## Kruskal-Wallis chi-squared = 6.6731, df = 2, p-value = 0.03556 \\(H_0:\\mu_1=\\mu_2=\\mu_3\\) (The populations are identical) \\(H_A:\\mu_1,\\mu_2,\\mu_3\\) are not equal \\(\\alpha=0.05\\) Reject the null hypothesis if \\(\\chi^2 \\ge 5.991\\), which is the value of \\(\\chi_{0.05}^2\\) for \\(3-1=2\\) degrees of freedom. From Kruskal-Wallis Test, \\(\\chi^2 \\approx 6.6731\\). Since \\(\\chi^2 \\approx 6.6731\\) is greater than 5.991, the null hypothesis must be rejected; we conclude that three methods of instruction are not all equally effective. "],
["survival-analysis.html", "§ Chapter 8 Survival Analysis 8.1 Kaplan-Meier estimate 8.2 Cox PH models", " § Chapter 8 Survival Analysis 8.1 Kaplan-Meier estimate Data：Every data set used is found in the package KMsurv, which are the data sets from Klein and Moeschberger’s book. install.packages(‘survival’) install.packages(‘KMsurv’) install.packages(‘DT’) install.packages(‘survminer’) This tongue data frame has 80 rows and 3 columns: type: Tumor DNA profile (1=Aneuploid Tumor, 2=Diploid Tumor) time: Time to death or on-study time, weeks delta: Death indicator (0=alive, 1=dead) data(tongue) DT::datatable(tongue, extensions = c(&#39;FixedColumns&#39;,&quot;FixedHeader&quot;), options = list(scrollX = TRUE, paging=TRUE, fixedHeader=TRUE)) attach(tongue) my.surv &lt;- Surv(time[type==1], delta[type==1]) my.surv ## [1] 1 3 3 4 10 13 13 16 16 24 26 27 28 30 ## [15] 30 32 41 51 65 67 70 72 73 77 91 93 96 100 ## [29] 104 157 167 61+ 74+ 79+ 80+ 81+ 87+ 87+ 88+ 89+ 93+ 97+ ## [43] 101+ 104+ 108+ 109+ 120+ 131+ 150+ 231+ 240+ 400+ my.fit &lt;- survfit(my.surv ~ 1 ) summary(my.fit) ## Call: survfit(formula = my.surv ~ 1) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 52 1 0.981 0.0190 0.944 1.000 ## 3 51 2 0.942 0.0323 0.881 1.000 ## 4 49 1 0.923 0.0370 0.853 0.998 ## 10 48 1 0.904 0.0409 0.827 0.988 ## 13 47 2 0.865 0.0473 0.777 0.963 ## 16 45 2 0.827 0.0525 0.730 0.936 ## 24 43 1 0.808 0.0547 0.707 0.922 ## 26 42 1 0.788 0.0566 0.685 0.908 ## 27 41 1 0.769 0.0584 0.663 0.893 ## 28 40 1 0.750 0.0600 0.641 0.877 ## 30 39 2 0.712 0.0628 0.598 0.846 ## 32 37 1 0.692 0.0640 0.578 0.830 ## 41 36 1 0.673 0.0651 0.557 0.813 ## 51 35 1 0.654 0.0660 0.537 0.797 ## 65 33 1 0.634 0.0669 0.516 0.780 ## 67 32 1 0.614 0.0677 0.495 0.762 ## 70 31 1 0.594 0.0683 0.475 0.745 ## 72 30 1 0.575 0.0689 0.454 0.727 ## 73 29 1 0.555 0.0693 0.434 0.709 ## 77 27 1 0.534 0.0697 0.414 0.690 ## 91 19 1 0.506 0.0715 0.384 0.667 ## 93 18 1 0.478 0.0728 0.355 0.644 ## 96 16 1 0.448 0.0741 0.324 0.620 ## 100 14 1 0.416 0.0754 0.292 0.594 ## 104 12 1 0.381 0.0767 0.257 0.566 ## 157 5 1 0.305 0.0918 0.169 0.550 ## 167 4 1 0.229 0.0954 0.101 0.518 plot(my.fit, main=&quot;Kaplan-Meier estimate with 95% confidence bounds&quot;, xlab=&quot;time&quot;, ylab=&quot;survival function&quot;) # hazard function H.hat&lt;--log(my.fit$surv) print(my.fit, print.rmean=TRUE) ## Call: survfit(formula = my.surv ~ 1) ## ## n events *rmean *se(rmean) median 0.95LCL ## 52.0 31.0 146.6 27.7 93.0 67.0 ## 0.95UCL ## NA ## * restricted mean with upper limit = 400 #compare type=1 and type=2 my.fit1&lt;-survfit(Surv(time,delta)~type, data=tongue) print(my.fit1) ## Call: survfit(formula = Surv(time, delta) ~ type, data = tongue) ## ## n events median 0.95LCL 0.95UCL ## type=1 52 31 93 67 NA ## type=2 28 22 42 23 112 ggsurvplot(my.fit1, pval = TRUE, conf.int = TRUE, risk.table = TRUE, ggtheme = theme_bw(), palette = c(&quot;#E7B800&quot;, &quot;#2E9FDF&quot;)) #Tests for two or more samples survdiff(Surv(time, delta) ~ type) # output omitted ## Call: ## survdiff(formula = Surv(time, delta) ~ type) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## type=1 52 31 36.6 0.843 2.79 ## type=2 28 22 16.4 1.873 2.79 ## ## Chisq= 2.8 on 1 degrees of freedom, p= 0.09 detach(tongue) 8.2 Cox PH models Mayo Clinic Primary Biliary Cirrhosis Data Followup of 312 randomised patients with primary biliary cirrhosis, a rare autoimmune liver disease, at Mayo Clinic. install.packages(‘JM’) fit &lt;- coxph(Surv(years, status2) ~ drug + sex + age, data = pbc2.id) summary(fit) ## Call: ## coxph(formula = Surv(years, status2) ~ drug + sex + age, data = pbc2.id) ## ## n= 312, number of events= 140 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## drugD-penicil -0.146013 0.864146 0.172143 -0.848 0.3963 ## sexfemale -0.470905 0.624437 0.221785 -2.123 0.0337 * ## age 0.042842 1.043773 0.008505 5.037 4.72e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## drugD-penicil 0.8641 1.1572 0.6167 1.2109 ## sexfemale 0.6244 1.6014 0.4043 0.9644 ## age 1.0438 0.9581 1.0265 1.0613 ## ## Concordance= 0.629 (se = 0.024 ) ## Rsquare= 0.101 (max possible= 0.991 ) ## Likelihood ratio test= 33.25 on 3 df, p=3e-07 ## Wald test = 34.87 on 3 df, p=1e-07 ## Score (logrank) test = 35.31 on 3 df, p=1e-07 Survival in patients with advanced lung cancer from the North Central Cancer Treatment Group. Performance scores rate how well the patient can perform usual daily activities. DT::datatable(lung, extensions = c(&#39;FixedColumns&#39;,&quot;FixedHeader&quot;), rownames = FALSE, options = list(scrollX = TRUE, paging=TRUE, fixedHeader=TRUE)) survdiff(Surv(time,status==1)~sex, data=lung) ## Call: ## survdiff(formula = Surv(time, status == 1) ~ sex, data = lung) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## sex=1 138 26 35.6 2.60 6.23 ## sex=2 90 37 27.4 3.39 6.23 ## ## Chisq= 6.2 on 1 degrees of freedom, p= 0.01 # predict male survival from age and medical scores MaleMod &lt;- coxph(Surv(time,status)~age+ph.ecog+ph.karno+pat.karno, data=lung, subset=sex==1) # display results summary(MaleMod) ## Call: ## coxph(formula = Surv(time, status) ~ age + ph.ecog + ph.karno + ## pat.karno, data = lung, subset = sex == 1) ## ## n= 134, number of events= 108 ## (4 observations deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## age 0.022465 1.022719 0.012216 1.839 0.0659 . ## ph.ecog 0.665452 1.945370 0.225712 2.948 0.0032 ** ## ph.karno 0.025553 1.025883 0.011778 2.170 0.0300 * ## pat.karno -0.011059 0.989002 0.008892 -1.244 0.2136 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## age 1.023 0.9778 0.9985 1.048 ## ph.ecog 1.945 0.5140 1.2499 3.028 ## ph.karno 1.026 0.9748 1.0025 1.050 ## pat.karno 0.989 1.0111 0.9719 1.006 ## ## Concordance= 0.617 (se = 0.031 ) ## Rsquare= 0.125 (max possible= 0.998 ) ## Likelihood ratio test= 17.87 on 4 df, p=0.001 ## Wald test = 18.3 on 4 df, p=0.001 ## Score (logrank) test = 18.6 on 4 df, p=9e-04 plot(survfit(MaleMod)) Holding the other covariates constant, an additional year of age reduces the weekly hazard of rearrest by a factor of \\(e^{0.0225} = 1.022719\\) on average. Each ph.ecog increases the hazard by a factor of 1.945370 on average. The likelihood ratio test is a equivalent tests of the omnibus null hypothesis that all of the \\(\\beta&#39;s\\) are zero. In this instance, the hypothesis is soundly rejected. So we can obtain Cox proportional hazards model as follows: \\(h(t,x)=h_0(t)+exp(0.0225age+0.6655ph.ecog+0.0256ph.karno-0.011pat.karno)\\) "],
["generalized-estimating-equation.html", "§ Chapter 9 Generalized Estimating Equation", " § Chapter 9 Generalized Estimating Equation Data： These data are from a 1996 study (Gregoire, Kumar Everitt, Henderson and Studd) on the efficacy of estrogen patches in treating postnatal depression. Women were randomly assigned to either a placebo control group (group=0, n=27) or estrogen patch group (group=1, n=34). Prior to the first treatment all patients took the Edinburgh Postnatal Depression Scale (EPDS). EPDS data was collected monthly for six months once the treatment began. Higher scores on the EDPS are indicative of higher levels of depression. Depression scores greater than or equal to 11 were coded as 1. install.packages(“gee”) de &lt;- read.table(&quot;c://data/gee.txt&quot;, header=TRUE) library(gee) y &lt;- (de$depressd ~ de$visit + de$group ) gee( y, id=de$subj, family=binomial, corstr=&quot;exchangeable&quot;) ## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27 ## running glm to get initial regression estimate ## (Intercept) de$visit de$group ## 2.3836603 -0.4402142 -1.6066020 ## ## GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA ## gee S-function, version 4.13 modified 98/01/27 (1998) ## ## Model: ## Link: Logit ## Variance to Mean Relation: Binomial ## Correlation Structure: Exchangeable ## ## Call: ## gee(formula = y, id = de$subj, family = binomial, corstr = &quot;exchangeable&quot;) ## ## Number of observations : 295 ## ## Maximum cluster size : 6 ## ## ## Coefficients: ## (Intercept) de$visit de$group ## 2.4089588 -0.3985943 -1.6158979 ## ## Estimated Scale Parameter: 1.00449 ## Number of Iterations: 3 ## ## Working Correlation[1:4,1:4] ## [,1] [,2] [,3] [,4] ## [1,] 1.000000 0.449131 0.449131 0.449131 ## [2,] 0.449131 1.000000 0.449131 0.449131 ## [3,] 0.449131 0.449131 1.000000 0.449131 ## [4,] 0.449131 0.449131 0.449131 1.000000 ## ## ## Returned Error Value: ## [1] 0 "],
["references.html", "References", " References "]
]
